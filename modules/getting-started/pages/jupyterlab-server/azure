Integrate Workbench with Azure Machine Learning

This guide walks you through integrating the ML Workbench with a Compute Instance on Azure Machine Learning (AML).
1. Prerequisite

An Azure Machine Learning Workspace with a running Compute Instance.
Optional: SSH access to the Compute Instance to access apps from the sandbox container via local browser. Also improves access to Compute Instance via VS.Code.
2. Procedure

From Azure ML Studio, find your Compute Instance, and open the Jupyter Lab instance running on it.
Open a terminal in the Jupyter Lab instance.
From the terminal, run pip install tigergraph_mlworkbench. This installs the ML Workbench JupyterLab extension.
From the terminal, run the following command to install the tigergraph-torch Python kernel. Choose the appropriate command depending on whether you are using a CPU or GPU for training:
- CPU: conda env create -f https://raw.githubusercontent.com/TigerGraph-DevLabs/mlworkbench-docs/main/conda_envs/tigergraph-torch-cpu.yml
- GPU: conda env create -f https://raw.githubusercontent.com/TigerGraph-DevLabs/mlworkbench-docs/main/conda_envs/tigergraph-torch-gpu.yml
Install the Tigergraph Pytorch kernel in JupyterLab by running the following command in the terminal: conda activate tigergraph-torch-gpu && python -m ipykernel install --user --name tigergraph-torch-gpu --display-name "TigerGraph Pytorch (gpu)"
Once installation finishes, refresh your browser. You should see a small TigerGraph logo on the very left navigation bar and a new Python kernel called TigerGraph Pytorch on the launch page.
3. Next steps

(A) With the ML Workbench JupyterLab extension and the tigergraph-torch kernel installed, the next step is to deploy GDPS on your TigerGraph instance so the Workbench can communicate with your TigerGraph database.
 
(B) As alternative, you can also use the ML Workbench Sandbox image for providing the TigerGraph instance. The sandbox image has a TigerGraph instance and GDPS already preinstalled.
 
To run the ML Workbench Sandbox image on the Compute Instance, open a terminal on the Compute Instance and run the following command:
docker run -it -p 14022:22 -p 8000:8000 -p 8900:8888 -p 9000:9000 -p 14240:14240 -p 6006:6006 --name tgsandbox --ulimit nofile=1000000:1000000 tigergraphml/sandbox
 
This will run the sandbox container on the Compute Instance and make its TigerGraph instance incl. GDPS available to the Compute Instance as if it was running locally on it. Note that the container will not automatically restart after a reboot of the Compute Instance. To have the container automatically restart, add a --restart unless-stopped parameter.
 
To transfer files to the sandbox container, the simplest way is to add a  -v <path on compute instance>:/home/tigergraph/tgsandbox/save parameter to the docker run command above. This will mount the specified folder on your Compute Instance directly into the sandbox container. To mount data for example from an Azure Data Lake, you can use the Data tab in the details UI of the Compute Instance.
 
To access GraphStudio and/or ML Workbench on the sandbox container, use can use SSH port forwarding. For example, running ssh -i … azureuser@<compute instance address> -p … -L 14240:127.0.0.1:14240 -L 8888:127.0.0.1:8900 allows you to access GraphStudio and the ML Workbench living in the sandbox from your local browser at localhost:14240 (GraphStudio) and localhost:8888 (ML Workbench). To use SSH, you need to enable SSH when you create the Compute Instance.
 
4. VS.Code

To access and use the Compute Instance from Visual Studio Code (VS.Code), go to the details page of your Compute Instance in AML Studio and open the Connect pane. It will give you all details required to connect to your Compute Instance using SSH.
 
With that information
open VS.Code
ensure that you have the "Azure Machine Learning" extension installed and
connect your local VS.Code to the Compute Instance by clicking on the green button on the bottom left (see here for details).
 
As a result, a separate VS.Code instance will open. That instance uses the Compute Instance as backend, for example all terminals, scripts, Jupyter kernels etc. will run on the Compute Instance.
 
By default, no port forwardings preconfigured. If you need port forwardings, either use the VS.Code UI or edit your SSH's config file (in your SSH's directory). For example, the following forwards all relevant TigerGraph ports to the sandbox container.
 
        Host <host ip>
          HostName <host ip>
          User azureuser
          Port 50000
          ForwardAgent yes
          LocalForward 6006 127.0.0.1:6006
          LocalForward 8000 127.0.0.1:8000
          LocalForward 8888 127.0.0.1:8900
          LocalForward 9000 127.0.0.1:9000
          LocalForward 14022 127.0.0.1:14022
          LocalForward 14240 127.0.0.1:14240
 
 
 
